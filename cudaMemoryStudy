regisiter：
for (int i = 0; i < size; ++i)
{
      sum += array[i];
}
如果sum不写在核函数里，而是在内存里，那么这个sum就要写size次，而如果当做局部变量，只是把最终结果写回到内存，则只进行一次读写，quickly！
int sum=0；
for (int i = 0; i < size; ++i)
      sum += array[i];
memSum=sum；//memSum是全局内存上的sum，只把最终结果写上去。把寄存器当做临时结果的存储。

Shared memory
https://blog.csdn.net/NYG8945/article/details/52874225?utm_source=blogxgwz3

cuda二维数组的创建与拷贝
https://blog.csdn.net/yu132563/article/details/52658080


全局内存机制：
    CUDA执行模型的特征之一指令都是以线程束为单位进行发布和执行，存储操作也是如此。全局内存是通过缓存进行加载？？？？？？什么叫内存通过缓存加载。
    数据从全局内存到SM（stream-multiprocessor）的传输，会进行cache，如果cache命中了，下一次的访问的耗时将大大减少。 每个SM都具有单独的L1 cache，
所有的SM共用一个L2 cache。 在计算能力2.x之前的设备，全局内存的访问会在L1\L2 cache上缓存；在计算能力3.x以上的设备，全局内存的访问只在L2 cache上缓存。
对于L1 cache，每次按照128字节进行缓存；对于L2 cache，每次按照32字节进行缓存。 这个128字节访问和32字节访问是需要设置吗？？？？？默认是哪个？？？？？？
__Idg（）函数好像可以强制32字节取。    
    注意：在Fermi架构的机器上，L1缓存是默认参与对全局的加载（即128字节）。后面的架构默认是不启用L1缓存的，如果要使用，使用编译指令：-Xptxas -dlcm=ca. 
禁用则使用-Xptxas -dlcm=cg。
    对于L1 cache，内存块大小支持32字节、64字节以及128字节，内存基于线程束来合并，分别表示线程束中每个线程以一个字节（1*32=32）、2字节（2*32=64）、4字节（4*32=128）
为单位读取数据。前提是，访问必须连续，并且访问的地址是以32字节对齐。
    合并访问的两个条件：1、对齐的内存，所以opencv的gpuMat是对齐内存块，方便合并的方式从CPU往GPU中传输，和从全局内存中往共享内存输。
                      2、每个线程以1,2,4字节的连续。 若一个线程读一个结构体x，y，z的数据，虽然结构体之间是连续的内存，但是每个线程需要的是12个字节，不是1、2、4了
    对于L2 cache，合并访存的字节减少为32字节，那么L2 cache相对L1 cache的好处？ 在非对齐访问、分散访问（非连续访问）的情况下，提高吞吐量（cache的带宽利用率）
    非对齐与非合并  https://blog.csdn.net/qq_17239003/article/details/79038333
    对于128字节加载，因为缓存是加载一批数据（此处128字节）不是只加载1个数据，所以非对齐方式访问，非对齐情况所需数据存在2个128上，要访问两次，对齐的话1次就够。
一次128字节的内存事物处理即可完成。 一次内存事务处理的时间？？？？？？？？？？？？？

共享内存
https://blog.csdn.net/Canhui_WANG/article/details/52709666  共享内存bank原理和解决办法，清晰简洁
https://face2ai.com/CUDA-F-4-3-%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F/      高精，看过的最好的对齐与合并，ldg指令
    共享内存带宽1.5T/S，全局内存190G/S（共享内存带宽更宽）；并且共享内存读取速度也快。
    但是可能有bank冲突：bank是共享内存分成的大小相等的存储器模块，可以同时被访问。所以对n个不同bank的访问可以同时进行，大大提高带宽。但前提就是同一个
warp中的线程不能访问一个bank，访问一个bank就会有bank冲突（特例就是warp所有线程都访问一个bank的同一数据，会广播，只需一次命令），所以，
同一个warp中访问的数据最好放不同bank。（看郭QQ收藏帖子）。
    在冲突最严重的情况下，速度会比全局显存还慢，但是如果 half-warp 的线程访问同一地址的时候，会产生一次广播，其速度反而没有下降。在不发生 bank conflict 
时，访问共享存储器的速度与寄存器相同。
    因此，如果程序在存取 shared memory 的时候，使用以下的方式：
    int number = data[base + tid]; 那就不会有任何 bank conflict，可以达到最高的效率。即1线程对1bank内的数。
    但是，如果是以下的方式：
    int number = data[base + 4 * tid];
那么，thread 0 和 thread 4 就会存取到同一个 bank，thread 1 和 thread 5 也是同 样，这样就会造成 bank conflict。在这个例子中，一个 half warp 的
16 个 threads 会有四个 threads 存取同一个 bank，因此存取 share memory 的速度会变成原来的 1/4。
    一个重要的例外是，当多个 thread 存取到同一个 shared memory 的地址时，shared memory 可以将这个地址的 32 bits 数据「广播」到所有读取的 threads，因此不会造成 bank conflict。例如：
int number = data[3];这样不会造成 bank conflict，因为所有的 thread 都读取同一个地址的数据。
    很多时候 shared memory 的 bank conflict 可以透过修改数据存放的方式来解决。例如，以下的程序：
    data[tid] = global_data[tid];     这里data为共享内存
    int number = data[16 * tid];会造成严重的 bank conflict，为了避免这个问题，可以把数据的排列方式稍加修改，
    把存取方式改成：
    int row = tid / 16;
    int column = tid % 16;
    data[row * 17 + column] = global_data[tid];
    int number = data[17 * tid];这样就不会造成 bank conflict 了。

    for (int i = thrIdY - halfWindow; i < thrIdY + halfWindow + 1; i++) {
			for (int j = thrIdX - halfWindow; j < thrIdX + halfWindow+1; j++) {
				a = imgA[i][j];  以前是这么写的，会慢，先用寄存器给行存下来，减少了访问共享内存数量
				b = imgB[i][j];
				meanA += a;
				meanB += b;
				varA += a*a;
				varB += b*b;
				integAxB += a*b;
			}
		}
    
    
    for (int i = thrIdY - halfWindow; i < thrIdY + halfWindow + 1; i++) {
			curRowA = imgA[i];
			curRowB = imgB[i];
			for (int j = thrIdX - halfWindow; j < thrIdX + halfWindow+1; j++) {
				a = curRowA[j];
				b = curRowB[j];
				meanA += a;
				meanB += b;
				varA += a*a;
				varB += b*b;
				integAxB += a*b;
			}
		}
    https://www.cnblogs.com/ethan-li/p/10216641.html
    
常量内存
    适用于线程束内所有线程（不一定所有，大多数线程）访问相同数据。如果访问不同的数据可能效率会降低。解释如下：
    当常量内存将数据分配或广播到线程束中的每个线程时（注意，实际上硬件会将单次内存读取操作广播到半个线程束），广播能够在单个周期内发生，
因此这个特性是非常有用的。虽然当所有16个线程都读取相同地址时，这个功能可以极大提高性能，但当所有16个线程分别读取不同的地址时，它实际上会降低性能。
如果半个线程束中的所有16个线程需要访问常量内存中的不同数据，那么这个16次不同的读取操作会被串行化，从而需要16倍的时间来发出请求。但如果从全局内存中读取，
那么这些请求就会同时发出。这种情况下，从常量内存读取就会慢于从全局内存中读取。？？？？？？最多访问几个不同的数据会比全局内存快？？？



纹理介绍 ——————————————————————————————————————————————
https://blog.csdn.net/i_chaoren/article/details/78061379  好，有官方实例代码，还有cudaArray和线性显存绑定问纹理内存的区别。
https://blog.csdn.net/fb_help/article/details/82143156   纹理Texture所需参数详解
https://www.cnblogs.com/riddick/p/7892663.html  用cudaArray绑定2维纹理内存的实例，cudaArray是可以分配2维数组，也可以分1维，分1维数组时与普通的
cudaMalloc有什么区别？
https://blog.csdn.net/zhangpinghao/article/details/16811773 用cudaArray和cudaMallocPatch绑定2维纹理内存的实例。二维的不要用cudaMalloc出来的地址
，因为不是多大的2维纹理都能链接成功（如w 10 H 12），详见笔记本。

     纹理存储器（texture memory）是一种只读存储器，由GPU用于纹理渲染的的图形专用单元发展而来，因此也提供了一些特殊功能。

     纹理缓存的优势：纹理缓存具备硬件插值特性，可以实现最近邻插值和线性插值。纹理缓存针对二维空间的局部性访问进行了优化，所以通过纹理缓存访问二维矩阵的
邻域会获得加速。不需要满足全局内存的合并访问条件。

     纹理可以是一段连续的设备内存，也可以是一个CUDA数组（cudaMalloArray可以创建cuda数组）。但是CUDA数组对局部寻址有优化，称为“块线性”，原理是将邻域
元素缓存在同一条cache线上，这将加快邻域内的寻址，但是对于设备内存，并没有“块线性”。所以，选择采用CUDA数组，还是设备内存，需要根据实际情况决定，将数据
copy至CUDA数组是很耗时的。

     纹理的创建通过texture object或者texture reference，注意只有计算能力3.0以上的设备支持texture object，本文仅介绍texture reference。
     
使用3步走：——————————————————————————————————————————————
     1、声明全局纹理引用（也叫纹理参考）
     2、将显存绑定到纹理引用。cudaBindTexture（）或cudaBindTextureToArray（），两者不一样，见上面cuda数组的好处。
     3、从纹理参考中取值
     4、解绑
    
一步 纹理参考系的声明——————————————————————————————————————————————
     texture<DataType, Type, ReadMode> texRef;
     其中：
     -DataType指定纹理获取时的返回的数据类型，DataType限制为基本的整形和单精度浮点型，或向量类型（具体可查手册）。
     -Type指定纹理参考的类型，且等于cudaTextureType1D（一维纹理）,cudaTextureType2D（二维纹理）或cudaTextureType3D（三维纹理），
          或者cudaTextureType1Dlayered（一维层次纹理）或cudaTextureType2Dlayered（二维层次纹理），Type是可选的，默认为cudaTextureType1D；
     -ReadMode等于cudaReadModeNormalizedFloat或cudaReadModeElementType；如果它是cudaReadModeNormalizedFloat且DataType是16位或者8位整形，
          实际返回值是浮点类型，对于无符号整型，整形全范围被映射到[0.0，1.0]，对于有符号整型，映射成[-1.0，1.0]；例如，无符号八位值为0xff的纹理
          元素映射为1；如果ReadMode是cudaReadModeElementType，不会进行转换；ReadMode是个可选参数，默认为cudaReadModeElementType。

二步 纹理绑定 ——————————————————————————————————————————————
     channelDesc描述获取纹理时返回值的格式；channelDesc类型定义如下：
      struct cudaChannelFormatDesc{
        int x;     //通道0的数据位深度（比特数）
        int y;     //通道1的数据位深度
        int z;     //通道2的数据位深度
        int w;
        enum cudaChannelFormatKind f;     //数据类型
      };

     其中x、y、z和w 是返回值各组件的位数，而f为：
      -cudaChannelFormatKindSigned，如果这些组件是有符号整型；

      -cudaChannelFormatKindUnsigned，如果这些组件是无符号整型；

      -cudaChannelFormatKindFloat，如果这些组件是浮点类型。

     在内核中使用纹理参考从纹理存储器中读取数据之前，对于线性存储器必须使用 cudaBindTexture() 或cudaBindTexture2D()，
     对于CUDA数组，必须使用cudaBindTextureToArray()，将纹理参考绑定到纹理。
     cudaUnbindTexture()用于解绑定纹理参考。
     
三步 纹理拾取 ——————————————————————————————————————————————
     对于一维、二维和三维的CUDA数组绑定的纹理，分别使用tex1D( )、tex2D()和tex3D()函数访问，并且使用浮点型纹理坐标。在算法实现中，这相当于是线性插值。
     对与线性内存绑定的纹理，使用texfetch1D函数访问，采用的纹理坐标是整型。由cudaMallocPitch或者cudaMalloc3D分配的线性空间实际上仍然是经过填充、
对齐的一维线性空间，因此也用texfetch1D()函数访问。
     最简单的方法就是：使用tex1Dfetch() ，因为：仅支持整数地址；没有提供额外的过滤或地址模式。
     方法tex1D(), tex2D(), 和tex3D()的使用更为复杂，因为对纹理坐标的解释，在纹理拾取时会有怎样的运算，以及纹理拾取所传递的返回值都通过设定
纹理参照系的易变（运行时间）和不变（编译时间）属性得以控制。其参数如下：
      不变参数（编译时间）
      类型：当拾取时，返回类型
      基础整数和浮子式
      CUDA 1-, 2-, 4-元素向量
      维度：:
      现在是Currently 1D, 2D, 或3D
      读取模式：:
      cudaReadModeElementType
      cudaReadModeNormalizedFloat (对8- 或16-字节整数有效)
      返回[-1,1] （以签字）, [0,1] 未签字
      易变参数 (运行时间，仅适用于数组纹理和音高线性内存)
      规格化：:
      非零 = 地址范围[0, 1]
      过滤模式
      cudaFilterModePoint
      cudaFilterModeLinear
      地址模式
      cudaAddressModeClamp
      cudaAddressModeWrap
      如需了解更多信息，请参阅CUDA Programming Guide.（CUDA编程指南）
      在缺省状态下，在[0, N) 范围内（N是纹理大小，采用与坐标一致的维度），使用浮点坐标引用纹理。确定规格化纹理坐标会被使用，这意味着所有引用都在[0,1)范围内。
      Wrap模式确定界外地址会发生什么情况：
      Wrap: 界外座标被wrap(通过同余算法)
     
     
四步 纹理解绑 ——————————————————————————————————————————————
     cudaUnbindTexture()用于解绑定纹理参考。
   
   
官方实例代码：——————————————————————————————————————————————    
      // 2D float texture  必须声明为全局
      texture<float, cudaTextureType2D, cudaReadModeElementType> texRef;
      // Simple transformation kernel
      __global__ void transformKernel(float* output,int width, int height,float theta)
      {
          // Calculate normalized texture coordinates
          unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
          unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
          float u = x / (float)width;
          float v = y / (float)height;
          // Transform coordinates
          u -= 0.5f;
          v -= 0.5f;
          float tu = u * cosf(theta) - v * sinf(theta) + 0.5f;
          float tv = v * cosf(theta) + u * sinf(theta) + 0.5f;
          // Read from texture and write to global memory
          output[y * width + x] = tex2D(texRef, tu, tv);
      }

      // Host code
      int main()
      {
          // Allocate CUDA array in device memory
          cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc(32, 0, 0, 0,cudaChannelFormatKindFloat);
          cudaArray* cuArray;
          cudaMallocArray(&cuArray, &channelDesc, width, height);
           // Copy to device memory some data located at address h_data
          // in host memory
          cudaMemcpyToArray(cuArray, 0, 0, h_data, size,cudaMemcpyHostToDevice);
          // Set texture reference parameters
          texRef.addressMode[0] = cudaAddressModeWrap;
          texRef.addressMode[1] = cudaAddressModeWrap;
          texRef.filterMode = cudaFilterModeLinear;
          texRef.normalized = true;
          // Bind the array to the texture reference
          cudaBindTextureToArray(texRef, cuArray, channelDesc);
          // Allocate result of transformation in device memory
          float* output;
          cudaMalloc(&output, width * height * sizeof(float));
          // Invoke kernel
          dim3 dimBlock(16, 16);
          dim3 dimGrid((width + dimBlock.x - 1) / dimBlock.x,(height + dimBlock.y - 1) / dimBlock.y);
           transformKernel<<<dimGrid, dimBlock>>>(output, width, height,angle);
          // Free device memory
          cudaFreeArray(cuArray);
          cudaFree(output);
          return 0;
      }


    
